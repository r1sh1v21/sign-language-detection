{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497584cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: sklearn in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.1.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.1)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.53.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (22.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python sklearn matplotlib mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb78ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c79d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.90)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: comtypes in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyttsx3) (1.1.14)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\valee\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyttsx3) (306)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812dcac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "engine.say(\"jaffer is gay\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec78b0",
   "metadata": {},
   "source": [
    "mp holistic for keypoints extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee1b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120bda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_det(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f41b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stlyed_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b09c87",
   "metadata": {},
   "source": [
    "extracting keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048d63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61eac68",
   "metadata": {},
   "source": [
    "creating folders for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "666f8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('MP_Data2')\n",
    "actions = np.array(['hello','you','how'])\n",
    "no_sequences = 30\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "774f2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd11bb",
   "metadata": {},
   "source": [
    "data collecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adfb99ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length):\n\u001b[0;32m      8\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 10\u001b[0m     image, results \u001b[38;5;241m=\u001b[39m \u001b[43mmediapipe_det\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#print(results)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mmediapipe_det\u001b[1;34m(image, model)\u001b[0m\n\u001b[0;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      3\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "time.sleep(4)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                image, results = mediapipe_det(frame, holistic)\n",
    "                #print(results)\n",
    "\n",
    "                \n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),2,cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for {action} video number: {sequence}', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1,cv2.LINE_AA)\n",
    "                    cv2.waitKey(1000)\n",
    "\n",
    "                else:\n",
    "                    cv2.putText(image, f'Collecting frames for {action} video number: {sequence}', (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255),1,cv2.LINE_AA)\n",
    "                \n",
    "                draw_stlyed_landmarks(image, results)\n",
    "                \n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                \n",
    "                cv2.imshow(\"frame\", image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7608d105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6107305884361267"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d28dcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99ad60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f422aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764e3d4",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3e57009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65bc0789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8c0b1d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'you': 1, 'how': 2}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "388f3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e026415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80005c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "054cad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91ca6eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "17b7050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de65e",
   "metadata": {},
   "source": [
    "building and training LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bec25ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "88207813",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"Logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "03a7daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ed852650",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56ccced9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "4/4 [==============================] - 9s 288ms/step - loss: 4.8195 - categorical_accuracy: 0.1491\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 1s 215ms/step - loss: 57.9392 - categorical_accuracy: 0.2807\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 61.2598 - categorical_accuracy: 0.2807\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 1s 215ms/step - loss: 9.6124 - categorical_accuracy: 0.2281\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 24.4856 - categorical_accuracy: 0.1930\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 12.8312 - categorical_accuracy: 0.3158\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 1s 182ms/step - loss: 16.2380 - categorical_accuracy: 0.2632\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 10.8769 - categorical_accuracy: 0.2544\n",
      "Epoch 9/2000\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 10.4693 - categorical_accuracy: 0.2368\n",
      "Epoch 10/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 12.1633 - categorical_accuracy: 0.2982\n",
      "Epoch 11/2000\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 10.0295 - categorical_accuracy: 0.2105\n",
      "Epoch 12/2000\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 5.1351 - categorical_accuracy: 0.3509\n",
      "Epoch 13/2000\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 9.9812 - categorical_accuracy: 0.2719\n",
      "Epoch 14/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 11.6630 - categorical_accuracy: 0.2368\n",
      "Epoch 15/2000\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 5.6869 - categorical_accuracy: 0.1842\n",
      "Epoch 16/2000\n",
      "4/4 [==============================] - 1s 159ms/step - loss: 6.0182 - categorical_accuracy: 0.2544\n",
      "Epoch 17/2000\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 4.3131 - categorical_accuracy: 0.2193\n",
      "Epoch 18/2000\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 5.7033 - categorical_accuracy: 0.2632\n",
      "Epoch 19/2000\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 5.1927 - categorical_accuracy: 0.2544\n",
      "Epoch 20/2000\n",
      "4/4 [==============================] - 1s 161ms/step - loss: 3.7936 - categorical_accuracy: 0.2807\n",
      "Epoch 21/2000\n",
      "4/4 [==============================] - 1s 159ms/step - loss: 1.9087 - categorical_accuracy: 0.2544\n",
      "Epoch 22/2000\n",
      "4/4 [==============================] - 1s 160ms/step - loss: 1.5020 - categorical_accuracy: 0.2544\n",
      "Epoch 23/2000\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 1.4061 - categorical_accuracy: 0.2719\n",
      "Epoch 24/2000\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 1.4259 - categorical_accuracy: 0.2368\n",
      "Epoch 25/2000\n",
      "4/4 [==============================] - 1s 167ms/step - loss: 1.4300 - categorical_accuracy: 0.2544\n",
      "Epoch 26/2000\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.4135 - categorical_accuracy: 0.2544\n",
      "Epoch 27/2000\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.3956 - categorical_accuracy: 0.2544\n",
      "Epoch 28/2000\n",
      "4/4 [==============================] - 1s 183ms/step - loss: 1.3855 - categorical_accuracy: 0.2456\n",
      "Epoch 29/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.3827 - categorical_accuracy: 0.2982\n",
      "Epoch 30/2000\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.3851 - categorical_accuracy: 0.2544\n",
      "Epoch 31/2000\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 1.3820 - categorical_accuracy: 0.2807\n",
      "Epoch 32/2000\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 1.3851 - categorical_accuracy: 0.2895\n",
      "Epoch 33/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.3822 - categorical_accuracy: 0.2719\n",
      "Epoch 34/2000\n",
      "4/4 [==============================] - 1s 190ms/step - loss: 1.3793 - categorical_accuracy: 0.2719\n",
      "Epoch 35/2000\n",
      "4/4 [==============================] - 1s 198ms/step - loss: 1.3797 - categorical_accuracy: 0.2544\n",
      "Epoch 36/2000\n",
      "4/4 [==============================] - 1s 190ms/step - loss: 1.3759 - categorical_accuracy: 0.2895\n",
      "Epoch 37/2000\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.3751 - categorical_accuracy: 0.2719\n",
      "Epoch 38/2000\n",
      "4/4 [==============================] - 1s 182ms/step - loss: 1.3751 - categorical_accuracy: 0.4035\n",
      "Epoch 39/2000\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.3653 - categorical_accuracy: 0.3596\n",
      "Epoch 40/2000\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 1.3393 - categorical_accuracy: 0.3596\n",
      "Epoch 41/2000\n",
      "4/4 [==============================] - 1s 183ms/step - loss: 1.4420 - categorical_accuracy: 0.2807\n",
      "Epoch 42/2000\n",
      "4/4 [==============================] - 1s 192ms/step - loss: 1.3763 - categorical_accuracy: 0.2632\n",
      "Epoch 43/2000\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.3699 - categorical_accuracy: 0.2807\n",
      "Epoch 44/2000\n",
      "4/4 [==============================] - 1s 184ms/step - loss: 1.3656 - categorical_accuracy: 0.2895\n",
      "Epoch 45/2000\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.3347 - categorical_accuracy: 0.2982\n",
      "Epoch 46/2000\n",
      "4/4 [==============================] - 1s 192ms/step - loss: 1.2853 - categorical_accuracy: 0.4298\n",
      "Epoch 47/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.2277 - categorical_accuracy: 0.3772\n",
      "Epoch 48/2000\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 1.1487 - categorical_accuracy: 0.5175\n",
      "Epoch 49/2000\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 1.0524 - categorical_accuracy: 0.5263\n",
      "Epoch 50/2000\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.9975 - categorical_accuracy: 0.5789\n",
      "Epoch 51/2000\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.9062 - categorical_accuracy: 0.6053\n",
      "Epoch 52/2000\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 1.5195 - categorical_accuracy: 0.3333\n",
      "Epoch 53/2000\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 1.2209 - categorical_accuracy: 0.3772\n",
      "Epoch 54/2000\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 1.6079 - categorical_accuracy: 0.2982\n",
      "Epoch 55/2000\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 1.5110 - categorical_accuracy: 0.2281\n",
      "Epoch 56/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.3814 - categorical_accuracy: 0.2368\n",
      "Epoch 57/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.3997 - categorical_accuracy: 0.2895\n",
      "Epoch 58/2000\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 1.3799 - categorical_accuracy: 0.3772\n",
      "Epoch 59/2000\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 1.3687 - categorical_accuracy: 0.3158\n",
      "Epoch 60/2000\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.3415 - categorical_accuracy: 0.3860\n",
      "Epoch 61/2000\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.3541 - categorical_accuracy: 0.3333\n",
      "Epoch 62/2000\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 1.3417 - categorical_accuracy: 0.2544\n",
      "Epoch 63/2000\n",
      "4/4 [==============================] - 1s 183ms/step - loss: 1.3296 - categorical_accuracy: 0.2632\n",
      "Epoch 64/2000\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.3198 - categorical_accuracy: 0.3246\n",
      "Epoch 65/2000\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 1.2792 - categorical_accuracy: 0.3684\n",
      "Epoch 66/2000\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.3128 - categorical_accuracy: 0.4035\n",
      "Epoch 67/2000\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 1.2669 - categorical_accuracy: 0.4649\n",
      "Epoch 68/2000\n",
      "4/4 [==============================] - 1s 187ms/step - loss: 1.2380 - categorical_accuracy: 0.5175\n",
      "Epoch 69/2000\n",
      "4/4 [==============================] - 1s 181ms/step - loss: 1.2906 - categorical_accuracy: 0.4035\n",
      "Epoch 70/2000\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 1.3294 - categorical_accuracy: 0.3333\n",
      "Epoch 71/2000\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 1.3103 - categorical_accuracy: 0.3158\n",
      "Epoch 72/2000\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 1.2710 - categorical_accuracy: 0.4123\n",
      "Epoch 73/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 172ms/step - loss: 1.2832 - categorical_accuracy: 0.3772\n",
      "Epoch 74/2000\n",
      "4/4 [==============================] - 1s 183ms/step - loss: 1.2414 - categorical_accuracy: 0.3772\n",
      "Epoch 75/2000\n",
      "4/4 [==============================] - 1s 174ms/step - loss: 1.1975 - categorical_accuracy: 0.5351\n",
      "Epoch 76/2000\n",
      "4/4 [==============================] - 1s 164ms/step - loss: 1.2051 - categorical_accuracy: 0.4912\n",
      "Epoch 77/2000\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 1.1713 - categorical_accuracy: 0.5789\n",
      "Epoch 78/2000\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 1.1720 - categorical_accuracy: 0.5263\n",
      "Epoch 79/2000\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 1.1654 - categorical_accuracy: 0.5526\n",
      "Epoch 80/2000\n",
      "4/4 [==============================] - 1s 191ms/step - loss: 1.1295 - categorical_accuracy: 0.5614\n",
      "Epoch 81/2000\n",
      "4/4 [==============================] - 1s 173ms/step - loss: 1.1121 - categorical_accuracy: 0.5614\n",
      "Epoch 82/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.1325 - categorical_accuracy: 0.5439\n",
      "Epoch 83/2000\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 1.1284 - categorical_accuracy: 0.4386\n",
      "Epoch 84/2000\n",
      "4/4 [==============================] - 1s 189ms/step - loss: 1.0410 - categorical_accuracy: 0.5965\n",
      "Epoch 85/2000\n",
      "4/4 [==============================] - 1s 165ms/step - loss: 1.1052 - categorical_accuracy: 0.5614\n",
      "Epoch 86/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 1.0800 - categorical_accuracy: 0.5351\n",
      "Epoch 87/2000\n",
      "4/4 [==============================] - 1s 170ms/step - loss: 1.0205 - categorical_accuracy: 0.5614\n",
      "Epoch 88/2000\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.9659 - categorical_accuracy: 0.5263\n",
      "Epoch 89/2000\n",
      "4/4 [==============================] - 1s 175ms/step - loss: 0.9033 - categorical_accuracy: 0.6228\n",
      "Epoch 90/2000\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.8396 - categorical_accuracy: 0.6228\n",
      "Epoch 91/2000\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.8555 - categorical_accuracy: 0.5789\n",
      "Epoch 92/2000\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.8926 - categorical_accuracy: 0.5263\n",
      "Epoch 93/2000\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.8333 - categorical_accuracy: 0.6053\n",
      "Epoch 94/2000\n",
      "4/4 [==============================] - 1s 172ms/step - loss: 0.8371 - categorical_accuracy: 0.5789\n",
      "Epoch 95/2000\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.9276 - categorical_accuracy: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d9104011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('finalact1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8c36e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2a22c1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b9617a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fa9f1701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a7491f60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e896f7a",
   "metadata": {},
   "source": [
    "evaluating using confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed156e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bb83207a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9957810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f91c9b71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 2]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 2]]], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7809b9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "970be393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mlt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d8b2331",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 2, 2) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cm_display \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mConfusionMatrixDisplay(confusion_matrix \u001b[38;5;241m=\u001b[39m multilabel_confusion_matrix(ytrue, yhat), display_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcm_display\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow() \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py:148\u001b[0m, in \u001b[0;36mConfusionMatrixDisplay.plot\u001b[1;34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[0m\n\u001b[0;32m    145\u001b[0m im_kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_im_kw, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mim_kw}\n\u001b[0;32m    146\u001b[0m text_kw \u001b[38;5;241m=\u001b[39m text_kw \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_ \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mimshow(cm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mim_kw)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    150\u001b[0m cmap_min, cmap_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_\u001b[38;5;241m.\u001b[39mcmap(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_\u001b[38;5;241m.\u001b[39mcmap(\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\__init__.py:1459\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1459\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1461\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1462\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1463\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5658\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   5659\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   5660\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   5661\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   5662\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5663\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 5665\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5666\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5668\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    711\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 2, 2) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = multilabel_confusion_matrix(ytrue, yhat), display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2559a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak(s):\n",
    "    engine.say(s)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958aa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('result1.mp4')\n",
    "ret, frame = cap.read()\n",
    "\n",
    "image, results = mediapipe_det(frame, holistic)        \n",
    "draw_stlyed_landmarks(image, results)\n",
    "cv2.sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55f899fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_sent(sentence):\n",
    "    s=''\n",
    "    if 'hello' in sentence:\n",
    "        s+='hello'\n",
    "    if 'how' and 'you' in sentence:\n",
    "        s+= ' how are you?'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a38bf",
   "metadata": {},
   "source": [
    "real time detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4eb897a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hello\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "you\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "how\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "how\n"
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.9\n",
    "qq=0\n",
    "\n",
    "cap = cv2.VideoCapture('result1.mp4')\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_det(frame, holistic)\n",
    "        #print(results)\n",
    "        \n",
    "        draw_stlyed_landmarks(image, results)\n",
    "        \n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    \n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "        \n",
    "        cv2.rectangle(image, (0,0), (640,40), (245,117,16), -1)\n",
    "        \n",
    "        cv2.imshow(\"frame\", image)\n",
    "        \n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "08547113",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9bc3567f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how you'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3dcb3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea9233fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speak(pred_sent(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
